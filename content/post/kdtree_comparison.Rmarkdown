---
title: "Comparison of Naive KNN and K-D Tree KNN"
summary: Comparing the effectiveness of two implementations of the KNN algorithm
date: 2019-08-06
authors: ["admin"]
tags: ["r", "algorithms"]
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
kd_tree <- function(data, classes, depth = 1){
  
  # Check if data is matrix or cvector
  if(class(data) == "matrix"){
    depth <- depth %% ncol(data)
    
    if(!depth){
      # In case depth is 0 
      depth <- ncol(data)
    }
  } else {
    depth <- depth %% length(data)
    
    if(!depth){
      # In case depth is 0 
      depth <- length(data)
    }
  }
  
  
  # If only one point, make node
  if(is.null(dim(data))){
    
    # Check to see if data is empty
    if(is.na(data[depth])){
      return(NULL)
    }
    
    # Else return node
    return(list(
      axis = depth,
      value = data[depth],
      point = data,
      class = classes,
      left = NULL,
      right = NULL
    ))
  }
  
  
  # Use sorting and integer division to find median
  # Need to ensure median corresponds to actual data point
  
  middle <- nrow(data) %/% 2
  data_order <- order(data[, depth])
  
  # Split data excluding median point
  left_data <- data[data_order[1:(middle-1)], ]
  right_data <- data[data_order[(middle+1):nrow(data)], ]
  middle_point <- data[data_order[middle], ]
  
  return(list(
    axis = depth,
    value = middle_point[depth],
    point = middle_point,
    class = classes[data_order[middle]],
    left = kd_tree(left_data, classes[data_order[1:(middle-1)]], depth + 1),
    right = kd_tree(right_data, classes[data_order[(middle+1):nrow(data)]], depth + 1)
  ))
  
}

euclidean_dist <- function(a, b){
  sqrt(sum((a-b)^2))
}

find_neighbor <- function(tree, point, best = list(class = NULL, dist = NULL)){
  
  # Go back up one if null node
  if(is.null(tree)){
    return(best)
  }
  
  if(point[tree$axis] < tree$value){
    best <- find_neighbor(tree$left, point, best)
  } else {
    best <- find_neighbor(tree$right, point, best)
  }
  

  # Compare current node to best or set it as best if none set
  if(is.null(best$dist) || euclidean_dist(tree$point, point) < best$dist){
    best <- list(class = tree$class,
                 dist = euclidean_dist(tree$point, point))
  }
  
  # Check to see if the other branch needs to be checked 
  
  if(abs(tree$value - point[tree$axis]) < best$dist){
    if(point[tree$axis] > tree$value){
      best <- find_neighbor(tree$left, point, best)
    } else {
      best <- find_neighbor(tree$right, point, best)
    }
  }
  
  best
}

nearest_neighbor_tree <- function(training, true, testing){
  neighbors <- numeric(nrow(testing))
  tree <- kd_tree(training, true)
  for(i in 1:nrow(testing)){
      
    neighbors[i] <- find_neighbor(tree, testing[i, ])$class
    
  }
  
  neighbors
}
  

nearest_neighbor_naive <- function(training, true, testing){
  neighbors <- numeric(nrow(testing))
  
  for(i in 1:nrow(testing)){
    min <- NULL
    neighbor <- NULL
    for(j in 1:nrow(training)){
      dist <- euclidean_dist(testing[i, ], training[j, ])
      if(is.null(min)|| dist < min){
        min <- dist
        neighbor <- j
      }
    }
    neighbors[i] <- true[neighbor]
    
  }
  
  neighbors
}
```

## Introduction

The K-Nearest Neighbor algorithm is one of the simplest machine learning algorithms to understand. You use a training set of data with labels, and then for each point in your testing set, you find the point in the training set that it is closest to, and use that training point's label to predict the testing point's label. There are modifications that can be made such as using a majority vote of the k closest points (hence the name) or weight the votes based on distance, but fundamentally it's all the same. It is an intuitive algorithm and one that is relatively simple to implement, but how it is implemented can make a huge difference in it's effectiveness which is what this post will explore. I will compare two different implementations, one using a naive approach and one using a data structure called a k-D tree which will I will explain later on.

## Naive Approach

The simplest way to find the point in the training set closest to one in the testing set is simply to compare the test point to every single point in the training set and decide which one is closest. It might already be apparent that the speed this algorithm runs is heavily dependent on how many points you have in each set. If you have n training points and m testing points then you have to make n*m comparisons, so the growth is exponential as your sets grow. 

### Implemenation

The first part of creating a KNN algorithm is to decide which distance metric to use for the comparisons. For this post, I'll use the most common distance, Euclidean distance. The R code which follows simply takes two vectors and calculates that distance.

```{r}
euclidean_dist <- function(a, b){
  sqrt(sum((a-b)^2))
}
```

Now that we have the distance metric, we can move on to the actual algorithm. As said before, this code just loops through every point in the testing set, and for each of those points, it loops through every point in the training set and finds which point has the smallest distance, then takes that point's classification as the testing point's classification. 

```{r, cache = T}
nearest_neighbor_naive <- function(training, classes, testing){
  classification <- character(nrow(testing))
  
  for(i in 1:nrow(testing)){
    min <- NULL
    neighbor <- NULL
    
    for(j in 1:nrow(training)){
      dist <- euclidean_dist(testing[i, ], training[j, ])
      if(is.null(min) || dist < min){
        min <- dist
        neighbor <- j
      }
    }
    
    classification[i] <- classes[neighbor]
    
  }
  
  classification
}
```

In order to time this algorithm, I will generate a training set with 10,000 points and 2 columns, classifying each point based on whether the sum of the two columns is greater than .7, and then use a testing set of 1000 points generated the same way and predict the classifications. 

```{r, cache = T}
set.seed(101)
n_train <- 1e5
n_test <- 1e3
train <- matrix(runif(n_train*2), nrow = n_train)
class <- ifelse(train[, 1] + train[, 2] > .7, "a", "b")
test <- matrix(runif(n_test*2), nrow = n_test)
truth <- ifelse(test[, 1] + test[, 2] > .7, "a", "b")

start <- Sys.time()
neighbors <- nearest_neighbor_naive(train, class, test)
end <- Sys.time()
end - start
```
```{r}
sum(neighbors != truth)/length(truth)
```
```{r, echo  = F}
old <- end - start
```

You can see this implementation took `r round(end-start, 1)` minutes! This is much too long and would only increase as we added points or variables. The misclassifcation rate was almost 0 (mistakes probably due to some points extremely close to the border) but that's little comfort knowing how long it takes. Luckily, there is a much more efficient way to do this.

## K-D Tree Approach

A k-D tree is a way of storing data that makes it much more efficient to search through the points and find the closest neighbor. The way a k-D tree works is, as the name might suggest, by storing the data in a tree. Starting with the root node, you pick the first column in the data and find the median value. Then you create two child branches, one with all the points whose value in the first column are less than the median value, and the other with all the points whose value in the first column are greater than the median value. The median point is stored at that node. Then for each of the child nodes you repeat the process using the second column instead. You keep repeating this process, cycling through the columns, until every point is stored at a node. For more information, check out the [Wikipedia](https://en.wikipedia.org/wiki/K-d_tree) post on the subect since it gives a very easy to understand description.

### Implementation

### Creating a K-D Tree

To start off, I'll run through the process of actually creating the tree itself and break it up into pieces.

The inputs to the tree are the training data set, the class labels for each point in the data set, and the current depth of the tree. This last argument is important since the tree is built recursively and we need to be able to keep track of the depth.

```{r, eval = F}
kd_tree <- function(data, classes, depth = 1)
```

Since this tree will likely have many layers of branches, we need to make sure the `depth` variable stays within the bounds of the number of columns, which can be achieved using the modulus operator

```{r, eval = F}
  # Check if data is matrix or a vector
  if(class(data) == "matrix"){
    depth <- depth %% ncol(data)
    
    if(!depth){
      # In case depth is 0 
      depth <- ncol(data)
    }
  } else {
    depth <- depth %% length(data)
    
    if(!depth){
      # In case depth is 0 
      depth <- length(data)
    }
  }
```

There are two ways of doing this depending on if the data is a matrix or a vector (which occurs when there is only one point left). There might be a more elegant way to due this but this will do for this example. I also reset the depth if it is 0 since R indices start at 1, but this wouldn't be an issue in other languages.

Since this is a recursive algorithm, we need to focus on the base case, when there is only one (or zero) data point passed to the function. Once this case is reached, we create a node, recording the point, it's median value, the axis of that median value, and set the right and left nodes to `NULL` since there are no more points to create child brances with.

```{r, eval = F}
  # If only one point, make node
  if(is.null(dim(data))){
    
    # Check to see if data is empty
    if(is.na(data[depth])){
      return(NULL)
    }
    
    # Else return node
    return(list(
      axis = depth,
      value = data[depth],
      point = data,
      class = classes,
      left = NULL,
      right = NULL
    ))
  }
```

Now that the base case has been taken care of, we can focus on the case when there are multiple points left. In this case, you use the current column and find the median value, then split the data up into left and right data sets. You'll notice I don't use R's `median` function even though that would be simpler. This is because I need to know the index of the median point so that I can store it in the node, and there would be an issue if I only had two points with values say 1 and 2 since the median would then be 1.5 and of little use. 

Once the data is split, I create a node just like above, but this time I create a left and right child branch by calling the `kd_tree` function again on the subsets of the data, increasing the depth for each by 1 as I do so. 

```{r, eval = F}
  # Use sorting and integer division to find median
  # Need to ensure median corresponds to actual data point
  
  middle <- nrow(data) %/% 2
  data_order <- order(data[, depth])
  
  # Split data excluding median point
  left_data <- data[data_order[1:(middle-1)], ]
  right_data <- data[data_order[(middle+1):nrow(data)], ]
  middle_point <- data[data_order[middle], ]
  
  return(list(
    axis = depth,
    value = middle_point[depth],
    point = middle_point,
    class = classes[data_order[middle]],
    left = kd_tree(left_data, classes[data_order[1:(middle-1)]], depth + 1),
    right = kd_tree(right_data, classes[data_order[(middle+1):nrow(data)]], depth + 1)
  ))
  
}

```

Now that the k-D tree is taken care of, we can move on to how to use it to find the nearest neighbor of a new point. 

#### Finding the Nearest Neighbor

The inputs to this function are the k-D tree created from the previous function, the point to find a neighbor for, and the current closest neighbor, both its class and distance from the testing point. To start off, both these values will be `NULL`.

```{r, eval = F}
find_neighbor <- function(tree, point, best = list(class = NULL, dist = NULL))
```

Since this is another recursive algorithm, we need to take care of the base case, which is when we get to a branch that doesn't exist. In this case, simply go back up one branch.

```{r, eval = F}
  # Go back up one if null node
  if(is.null(tree)){
    return(best)
  }
```

If the branch does exist, then we use the column of the point at the node and the median value to decide whether we should go down the left branch or the right branch, passing down the current closest neighbor as we go.

```{r, eval = F}
  if(point[tree$axis] < tree$value){
    best <- find_neighbor(tree$left, point, best)
  } else {
    best <- find_neighbor(tree$right, point, best)
  }
```

Once these functions have returned a value, we have to compare that value to the current node and see if it is closer than the current closest point.

```{r, eval = F}
  # Compare current node to best or set it as best if none set
  if(is.null(best$dist) || euclidean_dist(tree$point, point) < best$dist){
    best <- list(class = tree$class,
                 dist = euclidean_dist(tree$point, point))
  }
```

After checking the current node, we then have to decide if we need to head down the other branch we didn't take before to see if there is a closer point on that side. The Wikipedia article explains the theory behind this, but if the distance between the current node's value and the point's value is less than the distance between the point and it's current closest neighbor, there is a chance that a better candidate exists on that side.

```{r, eval = F}
  # Check to see if the other branch needs to be checked 
  
  if(abs(tree$value - point[tree$axis]) < best$dist){
    if(point[tree$axis] > tree$value){
      best <- find_neighbor(tree$left, point, best)
    } else {
      best <- find_neighbor(tree$right, point, best)
    }
  }
```

Once that is done, we can return the current best point and recursion will take care of the rest.

Now that the algorithm is complete, we can combine the two functions together into the full classifier and test it with the same data used before.

```{r}
nearest_neighbor_tree <- function(training, classes, testing){
  neighbors <- numeric(nrow(testing))
  tree <- kd_tree(training, classes)
  for(i in 1:nrow(testing)){
      
    neighbors[i] <- find_neighbor(tree, testing[i, ])$class
    
  }
  
  neighbors
}
```
```{r}
set.seed(101)
n_train <- 1e5
n_test <- 1e3
train <- matrix(runif(n_train*2), nrow = n_train)
class <- ifelse(train[, 1] + train[, 2] > .7, "a", "b")
test <- matrix(runif(n_test*2), nrow = n_test)
truth <- ifelse(test[, 1] + test[, 2] > .7, "a", "b")

start <- Sys.time()
neighbors <- nearest_neighbor_tree(train, class, test)
end <- Sys.time()
end - start
```
```{r}
sum(neighbors != truth)/length(truth)
```

This only took `r round(end - start, 3)` seconds! A `r round(100*(1 - (end-start)/(60*as.numeric(old))), 1)`% increase in speed! And there was no loss in accuracy. The speed benefits would only grow as the data grows more complex and larger. 

## Conclusion

Hopefully this example has shown that the way data is stored can be extremely important in how an algorithm performs. The accuracy is the same between them, so there is absolutely no reason to choose the naive implemenatation. The k-D tree approach might take more time to develop than the naive approach, but using this algorithm even just a dozen times will already make it worthwile. 



